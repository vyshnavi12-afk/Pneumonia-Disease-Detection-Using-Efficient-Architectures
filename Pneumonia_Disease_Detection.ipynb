{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "wLSI85-S0M71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "zip_file_path = '/content/drive/MyDrive/archive.zip'\n",
        "extract_path = '/content/dataset'\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# List the names of folders present in the extracted directory\n",
        "extracted_folders = [f for f in os.listdir(extract_path) if os.path.isdir(os.path.join(extract_path, f))]\n",
        "\n",
        "# Display the names of the extracted folders\n",
        "print(\"Extracted Folders:\")\n",
        "for folder in extracted_folders:\n",
        "    print(folder)"
      ],
      "metadata": {
        "id": "B0pu8Ikz2h86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Specify the path to the extracted folder\n",
        "extracted_folder_path = '/content/dataset/chest_xray'\n",
        "\n",
        "# Create DataFrames for each category (NORMAL and PNEUMONIA)\n",
        "def create_dataframe_from_category(category_path, label):\n",
        "    image_files = [f for f in os.listdir(category_path) if f.endswith('.jpeg') or f.endswith('.jpg') or f.endswith('.png')]\n",
        "    image_paths = [os.path.join(category_path, f) for f in image_files]\n",
        "    labels = [label] * len(image_paths)\n",
        "    df = pd.DataFrame({'image_path': image_paths, 'label': labels})\n",
        "    return df\n",
        "\n",
        "# Create DataFrames for the train, test, and validation sets\n",
        "train_normal_df = create_dataframe_from_category(os.path.join(extracted_folder_path, 'train', 'NORMAL'), label='NORMAL')\n",
        "train_pneumonia_df = create_dataframe_from_category(os.path.join(extracted_folder_path, 'train', 'PNEUMONIA'), label='PNEUMONIA')\n",
        "# Create DataFrames for the train, test, and validation sets\n",
        "test_normal_df = create_dataframe_from_category(os.path.join(extracted_folder_path, 'test', 'NORMAL'), label='NORMAL')\n",
        "test_pneumonia_df = create_dataframe_from_category(os.path.join(extracted_folder_path, 'test', 'PNEUMONIA'), label='PNEUMONIA')\n",
        "valid_normal_df = create_dataframe_from_category(os.path.join(extracted_folder_path, 'val', 'NORMAL'), label='NORMAL')\n",
        "valid_pneumonia_df = create_dataframe_from_category(os.path.join(extracted_folder_path, 'val', 'PNEUMONIA'), label='PNEUMONIA')\n",
        "\n",
        "\n",
        "# Display the first few rows of each DataFrame\n",
        "print(\"Train Normal DataFrame:\")\n",
        "print(train_normal_df.head())\n",
        "\n",
        "print(\"\\nTrain Pneumonia DataFrame:\")\n",
        "print(train_pneumonia_df.head())\n",
        "# Display the first few rows of each DataFrame\n",
        "print(\"Test Normal DataFrame:\")\n",
        "print(test_normal_df.head())\n",
        "\n",
        "print(\"\\nTest Pneumonia DataFrame:\")\n",
        "print(test_pneumonia_df.head())\n",
        "print(\"Valid Normal DataFrame:\")\n",
        "print(valid_normal_df.head())\n",
        "\n",
        "print(\"\\nValid Pneumonia DataFrame:\")\n",
        "print(valid_pneumonia_df.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "Q5kfJsXGiNgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install matplotlib\n"
      ],
      "metadata": {
        "id": "d3cbVk7Mjs6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.image import imread\n",
        "\n",
        "\n",
        "\n",
        "# Function to display images\n",
        "def display_images(data_frame, num_images=5):\n",
        "    for i in range(num_images):\n",
        "        image_path = data_frame.iloc[i]['image_path']\n",
        "        label = data_frame.iloc[i]['label']\n",
        "        image = imread(image_path)\n",
        "\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.imshow(image, cmap='gray')  # Assuming images are grayscale\n",
        "        plt.title(f'Label: {label}')\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "# Display images from the DataFrames\n",
        "print(\"Displaying Train Normal Images:\")\n",
        "display_images(train_normal_df)\n",
        "\n",
        "print(\"Displaying Train Pneumonia Images:\")\n",
        "display_images(train_pneumonia_df)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kPL7cOKhj0aL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display images from the DataFrames\n",
        "print(\"Displaying Test Normal Images:\")\n",
        "display_images(test_normal_df)\n",
        "\n",
        "print(\"Displaying Test Pneumonia Images:\")\n",
        "display_images(test_pneumonia_df)\n",
        "\n",
        "print(\"Displaying Valid Normal Images:\")\n",
        "display_images(valid_normal_df)\n",
        "\n",
        "print(\"Displaying Valid Pneumonia Images:\")\n",
        "display_images(valid_pneumonia_df)"
      ],
      "metadata": {
        "id": "ohmOtbdFkCBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image_path, target_size=(224, 224)):\n",
        "    image = imread(image_path)\n",
        "\n",
        "    # Apply data augmentation\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "    image = datagen.random_transform(image)\n",
        "\n",
        "    # Resize image\n",
        "    image = image_resize(image, target_size)\n",
        "\n",
        "    # Normalize pixel values\n",
        "    image = image.astype(np.float32) / 255.0\n",
        "\n",
        "    return image"
      ],
      "metadata": {
        "id": "pp6b9R9YoMza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create DataFrames for each category with preprocessing\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "def create_preprocessed_dataframe_from_category(category_path, label, target_size=(224, 224)):\n",
        "    image_files = [f for f in os.listdir(category_path) if f.endswith('.jpeg') or f.endswith('.jpg') or f.endswith('.png')]\n",
        "    image_paths = [os.path.join(category_path, f) for f in image_files]\n",
        "\n",
        "    preprocessed_images = []\n",
        "    for image_path in image_paths:\n",
        "        preprocessed_image = preprocess_image(image_path, target_size)\n",
        "        preprocessed_images.append(preprocessed_image)\n",
        "\n",
        "    labels = [label] * len(preprocessed_images)\n",
        "    df = pd.DataFrame({'image_path': image_paths, 'label': labels})\n",
        "    return df"
      ],
      "metadata": {
        "id": "IyuB5WGCo_sS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "8507-XQrzoel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image_path, target_size=(224, 224)):\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = cv2.resize(image, target_size, interpolation=cv2.INTER_CUBIC)\n",
        "    image = image.astype(np.float32) / 255.0\n",
        "    return image\n",
        "\n",
        "# Create DataFrames for the train, test, and validation sets with preprocessing\n",
        "train_normal_df = create_preprocessed_dataframe_from_category(os.path.join(extracted_folder_path, 'train', 'NORMAL'), label='NORMAL')\n",
        "train_pneumonia_df = create_preprocessed_dataframe_from_category(os.path.join(extracted_folder_path, 'train', 'PNEUMONIA'), label='PNEUMONIA')\n",
        "test_normal_df = create_preprocessed_dataframe_from_category(os.path.join(extracted_folder_path, 'test', 'NORMAL'), label='NORMAL')\n",
        "test_pneumonia_df = create_preprocessed_dataframe_from_category(os.path.join(extracted_folder_path, 'test', 'PNEUMONIA'), label='PNEUMONIA')\n",
        "valid_normal_df = create_preprocessed_dataframe_from_category(os.path.join(extracted_folder_path, 'val', 'NORMAL'), label='NORMAL')\n",
        "valid_pneumonia_df = create_preprocessed_dataframe_from_category(os.path.join(extracted_folder_path, 'val', 'PNEUMONIA'), label='PNEUMONIA')\n",
        "\n"
      ],
      "metadata": {
        "id": "RtYcTLi6qr_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "   # Function to display images\n",
        "def display_images(data_frame, num_images=5):\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    for i in range(num_images):\n",
        "        image_path = data_frame.iloc[i]['image_path']\n",
        "        label = data_frame.iloc[i]['label']\n",
        "        image = imread(image_path)\n",
        "\n",
        "        plt.subplot(1, num_images, i+1)\n",
        "        plt.imshow(image)\n",
        "        plt.title(label)\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Display images from the DataFrames\n",
        "print(\"Displaying Train Normal Images:\")\n",
        "display_images(train_normal_df)\n",
        "\n",
        "print(\"Displaying Train Pneumonia Images:\")\n",
        "display_images(train_pneumonia_df)\n",
        "\n",
        "print(\"Displaying Test Normal Images:\")\n",
        "display_images(test_normal_df)\n",
        "\n",
        "print(\"Displaying Test Pneumonia Images:\")\n",
        "display_images(test_pneumonia_df)\n",
        "\n",
        "print(\"Displaying Valid Normal Images:\")\n",
        "display_images(valid_normal_df)\n",
        "\n",
        "print(\"Displaying Valid Pneumonia Images:\")\n",
        "display_images(valid_pneumonia_df)"
      ],
      "metadata": {
        "id": "X30jbnExrckd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "num_components = 100  # Choose the number of principal components\n",
        "pca = PCA(n_components=num_components, random_state=42)\n",
        "train_images_preprocessed = np.array([preprocess_image(image_path) for image_path in train_normal_df['image_path']])\n",
        "valid_images_preprocessed = np.array([preprocess_image(image_path) for image_path in valid_normal_df['image_path']])\n",
        "test_images_preprocessed = np.array([preprocess_image(image_path) for image_path in test_normal_df['image_path']])\n",
        "\n",
        "train_images_flattened = train_images_preprocessed.reshape((len(train_images_preprocessed), -1))\n",
        "valid_images_flattened = valid_images_preprocessed.reshape((len(valid_images_preprocessed), -1))\n",
        "test_images_flattened = test_images_preprocessed.reshape((len(test_images_preprocessed), -1))\n",
        "train_images_pca = pca.fit_transform(train_images_flattened)\n",
        "valid_images_pca = pca.transform(valid_images_flattened)\n",
        "test_images_pca = pca.transform(test_images_flattened)\n",
        "\n",
        "# Display the explained variance ratio\n",
        "print(\"Explained variance ratio:\", sum(pca.explained_variance_ratio_))"
      ],
      "metadata": {
        "id": "61ydWl-er9vZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Convert labels to numeric format (0 or 1)\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels = label_encoder.fit_transform(train_normal_df['label'])\n",
        "valid_labels = label_encoder.transform(valid_normal_df['label'])\n",
        "test_labels = label_encoder.transform(test_normal_df['label'])"
      ],
      "metadata": {
        "id": "0BLY8abs026Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ConvModel"
      ],
      "metadata": {
        "id": "STBcN4ou_cLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.callbacks import EarlyStopping as early_stopping\n",
        "\n",
        "# Combine train and validation data\n",
        "X_train_all = np.concatenate((train_images_preprocessed, valid_images_preprocessed), axis=0)\n",
        "y_train_all = np.concatenate((train_labels, valid_labels), axis=0)\n",
        "\n",
        "# Split into train and validation sets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_all, y_train_all, test_size=0.2, random_state=42)\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "\n",
        "# Build the model\n",
        "model = Sequential([\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(224, 224, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    Dropout(0.5),\n",
        "    Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping\n",
        "early_stopping = EarlyStopping(patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    datagen.flow(X_train, y_train, batch_size=batch_size),\n",
        "    steps_per_epoch=len(X_train) // batch_size,\n",
        "    epochs=10,\n",
        "    validation_data=datagen.flow(X_valid, y_valid, batch_size=batch_size),  # Use data generator for validation\n",
        "    validation_steps=len(X_valid) // batch_size,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate the model on test set\n",
        "test_loss, test_acc = model.evaluate(datagen.flow(test_images_preprocessed, test_labels, batch_size=batch_size))\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_acc)\n"
      ],
      "metadata": {
        "id": "sC8hgVtp6lP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN MODEL"
      ],
      "metadata": {
        "id": "SKz1hifA_YIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, LSTM, Bidirectional, Reshape\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(224, 224, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    LSTM(64, kernel_regularizer=l2(0.001)),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
        "    steps_per_epoch=len(X_train) // batch_size,\n",
        "    epochs=20,  # Increase the number of epochs\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    callbacks=[EarlyStopping(patience=5), lr_scheduler])\n",
        "\n",
        "# Evaluate the model on test set\n",
        "test_images_reshaped = test_images_preprocessed.reshape(-1, timesteps, features)\n",
        "test_loss, test_acc = model.evaluate(test_images_reshaped, test_labels)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_acc)"
      ],
      "metadata": {
        "id": "F6UNmqGc-gic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *VGG16*"
      ],
      "metadata": {
        "id": "rC7rYfQsEVuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.applications import VGG16\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Combine train and validation data\n",
        "X_train_all = np.concatenate((train_images_preprocessed, valid_images_preprocessed), axis=0)\n",
        "y_train_all = np.concatenate((train_labels, valid_labels), axis=0)\n",
        "\n",
        "# Split into train and validation sets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_all, y_train_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load VGG16 pre-trained model\n",
        "vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze layers in VGG16\n",
        "for layer in vgg16.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a new model\n",
        "model = Sequential([\n",
        "    vgg16,\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu', kernel_regularizer=l2(0.001)),  # L2 regularization\n",
        "    Dropout(0.5),  # Dropout layer\n",
        "    Dense(256, activation='relu', kernel_regularizer=l2(0.001)),  # L2 regularization\n",
        "    Dropout(0.5),  # Dropout layer\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(patience=5, restore_best_weights=True)\n",
        "\n",
        "# Define ReduceLROnPlateau callback\n",
        "reduce_lr = ReduceLROnPlateau(factor=0.2, patience=3)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=32,\n",
        "    epochs=20,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(test_images_preprocessed, test_labels)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_acc)\n"
      ],
      "metadata": {
        "id": "xEVxMAnIQjuC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}